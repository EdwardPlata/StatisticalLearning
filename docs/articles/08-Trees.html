<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Tree-Based Methods â€¢ StatisticalLearning</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">StatisticalLearning</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/03-LinearRegression.html">ISLR 3: Linear Regression</a>
    </li>
    <li>
      <a href="../articles/04-Classification.html">ISLR 4: Classification</a>
    </li>
    <li>
      <a href="../articles/05-CV-Resampling-Methods.html">ISLR 5: Resampling Methods</a>
    </li>
    <li>
      <a href="../articles/06-Model-Selection.html">ISLR 6: Linear Model Selection and Regularization</a>
    </li>
    <li>
      <a href="../articles/07-Nonlinear.html">Moving Beyond Linearity</a>
    </li>
    <li>
      <a href="../articles/08-Trees.html">Tree-Based Methods</a>
    </li>
    <li>
      <a href="../articles/09-SVM.html">Support Vector Machines</a>
    </li>
    <li>
      <a href="../articles/10-Unsupervised.html">Unsupervised Learning</a>
    </li>
    <li>
      <a href="../articles/quiz-05-CV-Resampling-Methods.html">ISLR: Bootstrap quiz</a>
    </li>
    <li>
      <a href="../articles/quiz-07-Nonlinear-Functions-in-R.html">ISLR: Nonlinear functions quiz</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Tree-Based Methods</h1>
            
          </div>

    
    
<div class="contents">

<div id="decision-trees" class="section level2">
<h2 class="hasAnchor">
<a href="#decision-trees" class="anchor"></a>Decision Trees</h2>
<p>We will have a look at the <code>Carseats</code> data using the <code>tree</code>, <code>rpart</code>, and <code>party</code> packages in R, as in the lab in the book. We create a binary response variable <code>High</code> (for high sales), and we include it in the same dataframe.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ISLR)
<span class="kw">attach</span>(Carseats)

<span class="kw">hist</span>(Sales)</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-1-1.png" width="672"></p>
<p>Now we fit a tree to these data, and summarize and plot it. Notice that we have to <em>exclude</em> <code>Sales</code> from the right-hand side of the formula, because the response is derived from it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rpart)

High &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Sales <span class="op">&lt;=</span><span class="st"> </span><span class="dv">8</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)

Carseats &lt;-<span class="st"> </span><span class="kw">data.frame</span>(Carseats, High)

tree.carseats &lt;-<span class="st"> </span><span class="kw">rpart</span>(High<span class="op">~</span>.<span class="op">-</span>Sales, <span class="dt">data =</span> Carseats)</code></pre></div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(tree.carseats, <span class="dt">uniform =</span> <span class="ot">TRUE</span>)
<span class="kw">text</span>(tree.carseats, <span class="dt">pretty=</span><span class="dv">1</span>)</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-3-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(party)
tree_carseats &lt;-<span class="st"> </span><span class="kw">ctree</span>(High<span class="op">~</span>.<span class="op">-</span>Sales, <span class="dt">data =</span> Carseats)

<span class="kw">plot</span>(tree_carseats)</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-4-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(tree_carseats)</code></pre></div>
<pre><code>##     Length      Class       Mode 
##          1 BinaryTree         S4</code></pre>
<p>For a detailed summary of the tree, print it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tree.carseats</code></pre></div>
<pre><code>## n= 400 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 400 164 No (0.59000000 0.41000000)  
##     2) ShelveLoc=Bad,Medium 315  98 No (0.68888889 0.31111111)  
##       4) Price&gt;=92.5 269  66 No (0.75464684 0.24535316)  
##         8) Advertising&lt; 13.5 224  41 No (0.81696429 0.18303571)  
##          16) CompPrice&lt; 124.5 96   6 No (0.93750000 0.06250000) *
##          17) CompPrice&gt;=124.5 128  35 No (0.72656250 0.27343750)  
##            34) Price&gt;=109.5 107  20 No (0.81308411 0.18691589)  
##              68) Price&gt;=126.5 65   6 No (0.90769231 0.09230769) *
##              69) Price&lt; 126.5 42  14 No (0.66666667 0.33333333)  
##               138) Age&gt;=49.5 22   2 No (0.90909091 0.09090909) *
##               139) Age&lt; 49.5 20   8 Yes (0.40000000 0.60000000) *
##            35) Price&lt; 109.5 21   6 Yes (0.28571429 0.71428571) *
##         9) Advertising&gt;=13.5 45  20 Yes (0.44444444 0.55555556)  
##          18) Age&gt;=54.5 20   5 No (0.75000000 0.25000000) *
##          19) Age&lt; 54.5 25   5 Yes (0.20000000 0.80000000) *
##       5) Price&lt; 92.5 46  14 Yes (0.30434783 0.69565217)  
##        10) Income&lt; 57 10   3 No (0.70000000 0.30000000) *
##        11) Income&gt;=57 36   7 Yes (0.19444444 0.80555556) *
##     3) ShelveLoc=Good 85  19 Yes (0.22352941 0.77647059)  
##       6) Price&gt;=142.5 12   3 No (0.75000000 0.25000000) *
##       7) Price&lt; 142.5 73  10 Yes (0.13698630 0.86301370) *</code></pre>
<p>Lets create a training and test set (250,150) split of the 400 observations, grow the tree on the training set, and evaluate its performance on the test set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tree)
<span class="kw">set.seed</span>(<span class="dv">1011</span>)
train &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Carseats), <span class="dv">250</span>)
tree.carseats &lt;-<span class="st"> </span><span class="kw">tree</span>(High <span class="op">~</span><span class="st"> </span>.<span class="op">-</span>Sales, Carseats, <span class="dt">subset =</span> train)

<span class="kw">plot</span>(tree.carseats)
<span class="kw">text</span>(tree.carseats, <span class="dt">pretty=</span><span class="dv">0</span>)</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-6-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tree.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(tree.carseats, Carseats[<span class="op">-</span>train,], <span class="dt">type =</span> <span class="st">"class"</span>)
<span class="kw">with</span>(Carseats[<span class="op">-</span>train,], <span class="kw">table</span>(tree.pred,High))</code></pre></div>
<pre><code>##          High
## tree.pred No Yes
##       No  72  27
##       Yes 18  33</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dv">72</span><span class="op">+</span><span class="dv">33</span>)<span class="op">/</span><span class="dv">150</span></code></pre></div>
<pre><code>## [1] 0.7</code></pre>
<p>This tree was grown to full depth, and might be too variable. We now use CV to prune it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tree)
cv.carseats &lt;-<span class="st"> </span><span class="kw">cv.tree</span>(tree.carseats, <span class="dt">FUN =</span> prune.misclass)
cv.carseats</code></pre></div>
<pre><code>## $size
##  [1] 20 14 13 10  9  7  6  5  2  1
## 
## $dev
##  [1]  65  65  57  57  59  64  64  59  78 104
## 
## $k
##  [1]      -Inf  0.000000  1.000000  1.333333  2.000000  2.500000  4.000000
##  [8]  5.000000  9.000000 31.000000
## 
## $method
## [1] "misclass"
## 
## attr(,"class")
## [1] "prune"         "tree.sequence"</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(cv.carseats)</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-7-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prune.carseats &lt;-<span class="st"> </span><span class="kw">prune.misclass</span>(tree.carseats, <span class="dt">best=</span><span class="dv">13</span>)
<span class="kw">plot</span>(prune.carseats);<span class="kw">text</span>(prune.carseats, <span class="dt">pretty=</span><span class="dv">0</span>)</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-7-2.png" width="672"></p>
<p>Now lets evaluate this pruned tree on the test data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tree.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(prune.carseats, Carseats[<span class="op">-</span>train,], <span class="dt">type=</span><span class="st">"class"</span>)
<span class="kw">with</span>(Carseats[<span class="op">-</span>train, ], <span class="kw">table</span>(tree.pred, High))</code></pre></div>
<pre><code>##          High
## tree.pred No Yes
##       No  72  28
##       Yes 18  32</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dv">72</span><span class="op">+</span><span class="dv">32</span>)<span class="op">/</span><span class="dv">150</span></code></pre></div>
<pre><code>## [1] 0.6933333</code></pre>
<p>It has done about the same as our original tree. So pruning did not hurt us wrt misclassification errors, and gave us a simpler tree.</p>

<p><strong>Random Forests and Boosting</strong></p>
<p>These methods use trees as building blocks to build more complex models. Here we will use the Boston housing data to explore random forests and boosting. These data are in the <code>MASS</code> package. It gives housing values and other statistics in each of 506 suburbs of Boston based on a 1970 census.</p>
</div>
<div id="random-forests" class="section level2">
<h2 class="hasAnchor">
<a href="#random-forests" class="anchor"></a>Random Forests</h2>
<p>Random forests build lots of bushy trees, and then average them to reduce the variance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForest)</code></pre></div>
<pre><code>## randomForest 4.6-12</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)

?Boston
<span class="kw">dim</span>(Boston)</code></pre></div>
<pre><code>## [1] 506  14</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">101</span>)
train &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Boston), <span class="dv">300</span>)</code></pre></div>
<p>Lets fit a random forest and see how well it performs. We will use the response <code>medv</code>, the median housing value (in $1K dollars)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rf.boston &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/randomForest/topics/randomForest">randomForest</a></span>(medv <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Boston, <span class="dt">subset =</span> train)
rf.boston</code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = medv ~ ., data = Boston, subset = train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 4
## 
##           Mean of squared residuals: 12.34243
##                     % Var explained: 85.09</code></pre>
<p>The MSR and % variance explained are based on OOB or <em>out-of-bag</em> estimates, a very clever device in random forests to get honest error estimates. The model reports that <code>mtry=4</code>, which is the number of variables randomly chosen at each split. Since <span class="math inline">\(p=13\)</span> here, we could try all 13 possible values of <code>mtry</code>. We will do so, record the results, and make a plot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">oob.err &lt;-<span class="st"> </span><span class="kw">double</span>(<span class="dv">13</span>)
test.err &lt;-<span class="st"> </span><span class="kw">double</span>(<span class="dv">13</span>)

<span class="cf">for</span>(mtry <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">13</span>){
                   fit &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/randomForest/topics/randomForest">randomForest</a></span>(medv <span class="op">~</span>., <span class="dt">data=</span>Boston, <span class="dt">subset=</span>train, <span class="dt">mtry=</span>mtry, <span class="dt">ntree=</span><span class="dv">400</span>)
         oob.err[mtry] &lt;-<span class="st"> </span>fit<span class="op">$</span>mse[<span class="dv">400</span>]
                  pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Boston[<span class="op">-</span>train,])
        test.err[mtry] &lt;-<span class="st"> </span><span class="kw">with</span>(Boston[<span class="op">-</span>train,], <span class="kw">mean</span>((medv<span class="op">-</span>pred)<span class="op">^</span><span class="dv">2</span>))
        <span class="kw">cat</span>(mtry,<span class="st">" "</span>)
}</code></pre></div>
<pre><code>## 1  2  3  4  5  6  7  8  9  10  11  12  13</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">matplot</span>(<span class="dv">1</span><span class="op">:</span>mtry, <span class="kw">cbind</span>(test.err,oob.err), <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">col=</span><span class="kw">c</span>(<span class="st">"red"</span>,<span class="st">"blue"</span>), <span class="dt">type=</span><span class="st">"b"</span>, <span class="dt">ylab=</span><span class="st">"Mean Squared Error"</span>)
<span class="kw">legend</span>(<span class="st">"topright"</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">"OOB"</span>,<span class="st">"Test"</span>), <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">col=</span><span class="kw">c</span>(<span class="st">"red"</span>,<span class="st">"blue"</span>))</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-11-1.png" width="672"></p>
<p>Not too difficult! Although the test-error curve drops below the OOB curve, these are estimates based on data, and so have their own standard errors (which are typically quite large). Notice that the points at the end with <code>mtry=13</code> correspond to bagging.</p>
</div>
<div id="boosting" class="section level2">
<h2 class="hasAnchor">
<a href="#boosting" class="anchor"></a>Boosting</h2>
<p>Boosting builds lots of smaller trees. Unlike random forests, each new tree in boosting tries to patch up the deficiencies of the current ensemble.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gbm3)
boost.boston &lt;-<span class="st"> </span><span class="kw">gbm</span>(medv <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Boston[train, ], <span class="dt">distribution =</span> <span class="st">"gaussian"</span>, 
    <span class="dt">n.trees =</span> <span class="dv">10000</span>, <span class="dt">shrinkage =</span> <span class="fl">0.01</span>, <span class="dt">interaction.depth =</span> <span class="dv">4</span>)

<span class="kw">summary</span>(boost.boston)</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-12-1.png" width="672"></p>
<pre><code>##             var    rel_inf
## rm           rm 35.1573249
## lstat     lstat 32.4749785
## dis         dis  7.9656899
## crim       crim  5.3735504
## nox         nox  5.0503321
## ptratio ptratio  3.5460553
## black     black  3.2374385
## age         age  2.9799571
## tax         tax  1.5512722
## chas       chas  1.2211187
## rad         rad  0.7399728
## indus     indus  0.5820085
## zn           zn  0.1203010</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(boost.boston, <span class="dt">var_index =</span> <span class="st">"lstat"</span>)</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-12-2.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(boost.boston, <span class="dt">var_index =</span> <span class="st">"rm"</span>)</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-12-3.png" width="672"></p>
<p>Lets make a prediction on the test set. With boosting, the number of trees is a tuning parameter, and if we have too many we can overfit. So we should use cross-validation to select the number of trees. We will leave this as an exercise. Instead, we will compute the test error as a function of the number of trees, and make a plot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n.trees &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">100</span>, <span class="dt">to=</span><span class="dv">10000</span>, <span class="dt">by=</span><span class="dv">100</span>)
predmat &lt;-<span class="st"> </span><span class="kw">predict</span>(boost.boston, <span class="dt">newdata =</span> Boston[<span class="op">-</span>train, ], <span class="dt">n.trees=</span>n.trees)
<span class="kw">dim</span>(predmat)</code></pre></div>
<pre><code>## [1] 206 100</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">berr &lt;-<span class="st"> </span><span class="kw">with</span>(Boston[<span class="op">-</span>train, ], <span class="kw">apply</span>((predmat<span class="op">-</span>medv)<span class="op">^</span><span class="dv">2</span>,<span class="dv">2</span>,mean))
<span class="kw">plot</span>(n.trees, berr, <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">ylab=</span><span class="st">"Mean Squared Error"</span>, <span class="dt">xlab=</span><span class="st">"# Trees"</span>, <span class="dt">main=</span><span class="st">"Boosting Test Error"</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">min</span>(test.err), <span class="dt">col=</span><span class="st">"red"</span>)</code></pre></div>
<p><img src="08-Trees_files/figure-html/unnamed-chunk-13-1.png" width="672"></p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#decision-trees">Decision Trees</a></li>
      <li><a href="#random-forests">Random Forests</a></li>
      <li><a href="#boosting">Boosting</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Justin M. Shea.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
