<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Support Vector Machines • StatisticalLearning</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">StatisticalLearning</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/03-LinearRegression.html">ISLR 3: Linear Regression</a>
    </li>
    <li>
      <a href="../articles/04-Classification.html">ISLR 4: Classification</a>
    </li>
    <li>
      <a href="../articles/05-CV-Resampling-Methods.html">ISLR 5: Resampling Methods</a>
    </li>
    <li>
      <a href="../articles/06-Model-Selection.html">ISLR 6: Linear Model Selection and Regularization</a>
    </li>
    <li>
      <a href="../articles/07-Nonlinear.html">Moving Beyond Linearity</a>
    </li>
    <li>
      <a href="../articles/08-Trees.html">Tree-Based Methods</a>
    </li>
    <li>
      <a href="../articles/09-SVM.html">Support Vector Machines</a>
    </li>
    <li>
      <a href="../articles/10-Unsupervised.html">Unsupervised Learning</a>
    </li>
    <li>
      <a href="../articles/quiz-05-CV-Resampling-Methods.html">ISLR: Bootstrap quiz</a>
    </li>
    <li>
      <a href="../articles/quiz-07-Nonlinear-Functions-in-R.html">ISLR: Nonlinear functions quiz</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Support Vector Machines</h1>
            
          </div>

    
    
<div class="contents">

<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>To demonstrate the SVM, it is easiest to work in low dimensions, so we can see the data.</p>
</div>
<div id="linear-svm-classifier" class="section level2">
<h2 class="hasAnchor">
<a href="#linear-svm-classifier" class="anchor"></a>Linear SVM classifier</h2>
<p>Lets generate some data in two dimensions, and make them a little separated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">10111</span>)
x &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">40</span>), <span class="dv">20</span>, <span class="dv">2</span>)
y &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">10</span>))

x[y<span class="op">==</span><span class="dv">1</span>, ] &lt;-<span class="st"> </span>x[y<span class="op">==</span><span class="dv">1</span>, ] <span class="op">+</span><span class="st"> </span><span class="dv">1</span>

<span class="kw">plot</span>(x, <span class="dt">col=</span>y<span class="op">+</span><span class="dv">3</span>, <span class="dt">pch=</span><span class="dv">19</span>)</code></pre></div>
<p><img src="09-SVM_files/figure-html/unnamed-chunk-1-1.png" width="672"></p>
<p>Now we will load the package <code>e1071</code> which contains the <code>svm</code> function we will use. We then compute the fit. Notice that we have to specify a <code>cost</code> parameter, which is a tuning parameter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x, <span class="dt">y=</span><span class="kw">as.factor</span>(y))
svmfit &lt;-<span class="st"> </span><span class="kw">svm</span>(y <span class="op">~</span>., <span class="dt">data =</span> dat, <span class="dt">kernel=</span><span class="st">"linear"</span>, <span class="dt">cost=</span><span class="dv">10</span>, <span class="dt">scale=</span><span class="ot">FALSE</span>)
<span class="kw">print</span>(svmfit)</code></pre></div>
<pre><code>## 
## Call:
## svm(formula = y ~ ., data = dat, kernel = "linear", cost = 10, 
##     scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  10 
##       gamma:  0.5 
## 
## Number of Support Vectors:  6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(svmfit, dat)</code></pre></div>
<p><img src="09-SVM_files/figure-html/unnamed-chunk-2-1.png" width="672"></p>
<p>As mentioned in the the chapter, the plot function is somewhat crude, and plots X2 on the horizontal axis (unlike what R would do automatically for a matrix). Lets see how we might make our own plot.</p>
<p>The first thing we will do is make a grid of values for X1 and X2. We will write a function to do that, in case we want to reuse it. It uses the handy function <code>expand.grid</code>, and produces the coordinates of <code>n*n</code> points on a lattice covering the domain of <code>x</code>. Having made the lattice, we make a prediction at each point on the lattice. We then plot the lattice, color-coded according to the classification. Now we can see the decision boundary.</p>
<p>The support points (points on the margin, or on the wrong side of the margin) are indexed in the <code>$index</code> component of the fit.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">make.grid &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">n=</span><span class="dv">75</span>){
                      grange &lt;-<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">2</span>, range)
                          x1 &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span>grange[<span class="dv">1</span>, <span class="dv">1</span>], <span class="dt">to=</span>grange[<span class="dv">2</span>, <span class="dv">1</span>], <span class="dt">length=</span>n)
                          x2 &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span>grange[<span class="dv">1</span>, <span class="dv">2</span>], <span class="dt">to=</span>grange[<span class="dv">2</span>, <span class="dv">2</span>], <span class="dt">length=</span>n)
                      <span class="kw">expand.grid</span>(<span class="dt">X1 =</span> x1, <span class="dt">X2 =</span> x2)
  }

xgrid &lt;-<span class="st"> </span><span class="kw">make.grid</span>(x)
ygrid &lt;-<span class="st"> </span><span class="kw">predict</span>(svmfit, xgrid)

<span class="kw">plot</span>(xgrid, <span class="dt">col=</span><span class="kw">c</span>(<span class="st">"red"</span>,<span class="st">"blue"</span>)[<span class="kw">as.numeric</span>(ygrid)], <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">cex=</span>.<span class="dv">2</span>)
<span class="kw">points</span>(x, <span class="dt">col=</span>y<span class="op">+</span><span class="dv">3</span>, <span class="dt">pch=</span><span class="dv">19</span>)
<span class="kw">points</span>(x[svmfit<span class="op">$</span>index, ], <span class="dt">pch=</span><span class="dv">5</span>, <span class="dt">cex=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="09-SVM_files/figure-html/unnamed-chunk-3-1.png" width="672"></p>
<p>The <code>svm</code> function is not too friendly, in that we have to do some work to get back the linear coefficients, as described in the text. Probably the reason is that this only makes sense for linear kernels, and the function is more general. Here we will use a formula to extract the coefficients; for those interested in where this comes from, have a look in chapter 12 of ESL (“Elements of Statistical Learning”).</p>
<p>We extract the linear coefficients, and then using simple algebra, we include the decision boundary and the two margins.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">t</span>(svmfit<span class="op">$</span>coefs)<span class="op">%*%</span>x[svmfit<span class="op">$</span>index, ])
beta0 &lt;-<span class="st"> </span>svmfit<span class="op">$</span>rho

<span class="kw">plot</span>(xgrid, <span class="dt">col=</span><span class="kw">c</span>(<span class="st">"red"</span>,<span class="st">"blue"</span>)[<span class="kw">as.numeric</span>(ygrid)], <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">cex=</span>.<span class="dv">2</span>)
<span class="kw">points</span>(x, <span class="dt">col=</span>y<span class="op">+</span><span class="dv">3</span>, <span class="dt">pch=</span><span class="dv">19</span>)
<span class="kw">points</span>(x[svmfit<span class="op">$</span>index, ], <span class="dt">pch=</span><span class="dv">5</span>, <span class="dt">cex=</span><span class="dv">2</span>)
<span class="kw">abline</span>(beta0<span class="op">/</span>beta[<span class="dv">2</span>], <span class="op">-</span>beta[<span class="dv">1</span>]<span class="op">/</span>beta[<span class="dv">2</span>])
<span class="kw">abline</span>((beta0<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>beta[<span class="dv">2</span>], <span class="op">-</span>beta[<span class="dv">1</span>]<span class="op">/</span>beta[<span class="dv">2</span>], <span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">abline</span>((beta0<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span>beta[<span class="dv">2</span>], <span class="op">-</span>beta[<span class="dv">1</span>]<span class="op">/</span>beta[<span class="dv">2</span>], <span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="09-SVM_files/figure-html/unnamed-chunk-4-1.png" width="672"></p>
<p>Just like for the other models in this book, the tuning parameter <code>C</code> has to be selected. Different values will give different solutions. Rerun the code above, but using <code>C=1</code>, and see what we mean. One can use cross-validation to do this.</p>
</div>
<div id="nonlinear-svm" class="section level2">
<h2 class="hasAnchor">
<a href="#nonlinear-svm" class="anchor"></a>Nonlinear SVM</h2>
<p>Instead, we will run the SVM on some data where a non-linear boundary is called for. We will use the mixture data from ESL</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="kw">url</span>(<span class="st">"http://www.stanford.edu/~hastie/ElemStatLearn/datasets/ESL.mixture.rda"</span>))

<span class="kw">names</span>(ESL.mixture)</code></pre></div>
<pre><code>## [1] "x"        "y"        "xnew"     "prob"     "marginal" "px1"     
## [7] "px2"      "means"</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rm</span>(x, y)

<span class="kw">attach</span>(ESL.mixture)</code></pre></div>
<p>These data are also two dimensional. Lets plot them and fit a nonlinear SVM, using a radial kernel.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(x, <span class="dt">col =</span> y <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)</code></pre></div>
<p><img src="09-SVM_files/figure-html/unnamed-chunk-6-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y), x)

fit &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="kw">factor</span>(y) <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> dat, <span class="dt">scale =</span> <span class="ot">FALSE</span>, <span class="dt">kernel =</span> <span class="st">"radial"</span>, <span class="dt">cost =</span> <span class="dv">5</span>)</code></pre></div>
<p>Now we are going to create a grid, as before, and make predictions on the grid. These data have the grid points for each variable included on the data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xgrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">X1 =</span> px1, <span class="dt">X2 =</span> px2)

ygrid &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, xgrid)

<span class="kw">plot</span>(xgrid, <span class="dt">col =</span> <span class="kw">as.numeric</span>(ygrid), <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">cex =</span> <span class="fl">0.2</span>)
<span class="kw">points</span>(x, <span class="dt">col =</span> y<span class="op">+</span><span class="dv">1</span>, <span class="dt">pch =</span> <span class="dv">19</span>)</code></pre></div>
<p><img src="09-SVM_files/figure-html/unnamed-chunk-7-1.png" width="672"></p>
<p>We can go further, and have the predict function produce the actual function estimates at each of our grid points. We can include the actual decision boundary on the plot by making use of the contour function. On the dataframe is also <code>prob</code>, which is the true probability of class 1 for these data, at the gridpoints. If we plot its 0.5 contour, that will give us the <em>Bayes Decision Boundary</em>, which is the best one could ever do.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">func &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, xgrid, <span class="dt">decision.values =</span> <span class="ot">TRUE</span>)
func &lt;-<span class="st"> </span><span class="kw">attributes</span>(func)<span class="op">$</span>decision

xgrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">X1 =</span> px1, <span class="dt">X2 =</span> px2)
ygrid &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, xgrid)

<span class="kw">plot</span>(xgrid, <span class="dt">col =</span> <span class="kw">as.numeric</span>(ygrid), <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">cex =</span> <span class="fl">0.2</span>)
<span class="kw">points</span>(x, <span class="dt">col =</span> y <span class="op">+</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">pch =</span> <span class="dv">19</span>)

<span class="kw">contour</span>(px1, px2, <span class="kw">matrix</span>(func, <span class="dv">69</span>, <span class="dv">99</span>), <span class="dt">level =</span> <span class="dv">0</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)
<span class="kw">contour</span>(px1, px2, <span class="kw">matrix</span>(prob, <span class="dv">69</span>, <span class="dv">99</span>), <span class="dt">level =</span> <span class="fl">0.5</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="st">"blue"</span>, 
    <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="09-SVM_files/figure-html/unnamed-chunk-8-1.png" width="672"></p>
<p>We see in this case that the radial kernel has done an excellent job.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li><a href="#linear-svm-classifier">Linear SVM classifier</a></li>
      <li><a href="#nonlinear-svm">Nonlinear SVM</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Justin M. Shea.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
